# OpenAI Realtime API Compatible Server

[ä¸­æ–‡](README.md) | English

A local WebSocket server that mirrors the OpenAI Realtime API protocol, so you can swap OpenAI with local or thirdâ€‘party model providers while keeping the client mostly unchanged.

## âœ¨ Features

- ğŸ”„ **Protocol-compatible**: Mirrors OpenAI Realtime API style (URL, JSON events, audio encoding)
- ğŸ”Œ **Pluggable backends**: Uses an internal pipeline to connect STT/LLM/TTS providers (Deepgram, Ollama/Llama, ElevenLabs, SiliconFlow, etc.)
- ğŸš€ **Minimal client changes**: Usually only change `baseUrl` to point to this server
- ğŸ¤ **Built-in Server VAD**: Integrates VAD (Silero when available) for hands-free â€œopen micâ€ mode
- ğŸ™ï¸ **Terminal client included**: A full TUI client for voice interaction
- ğŸŒŸ **SiliconFlow supported**: Faster & cheaper in mainland China; see [SILICONFLOW.md](SILICONFLOW.md)

## ğŸ“ Project Structure

```
â”œâ”€â”€ main.py                 # FastAPI server entry
â”œâ”€â”€ config.py               # Config management (.env supported)
â”œâ”€â”€ logger_config.py        # Logging configuration module
â”œâ”€â”€ service_providers.py    # STT/LLM/TTS provider implementations
â”œâ”€â”€ protocol.py             # OpenAI Realtime API protocol definitions
â”œâ”€â”€ transport.py            # WebSocket Transport layer (protocol translator)
â”œâ”€â”€ pipeline_manager.py     # Pipeline manager
â”œâ”€â”€ realtime_session.py     # Session lifecycle manager
â”œâ”€â”€ audio_utils.py          # Audio utilities (resampling/playback, etc.)
â”œâ”€â”€ push_to_talk_app.py     # Terminal client (open-mic mode)
â”œâ”€â”€ test_client.py          # Simple test client
â””â”€â”€ requirements.txt        # Python dependencies
```

## ğŸš€ Quick Start

### 1) Install dependencies

```bash
# Option 1: create a venv (recommended)
python -m venv .venv

# Activate venv
# Windows PowerShell:
.\.venv\Scripts\Activate.ps1
# Windows CMD:
.venv\Scripts\activate.bat
# Linux/Mac:
source .venv/bin/activate

# Install dependencies
pip install -r requirements.txt
```

### 2) Configure services (important)

Copy and edit environment configuration:

```bash
cp .env.example .env
```

Recommended for users in mainland China (example):

```bash
LLM_PROVIDER=siliconflow
SILICONFLOW_API_KEY=your_api_key
SILICONFLOW_MODEL=deepseek-ai/DeepSeek-V3.2

TTS_PROVIDER=edge_tts
EDGE_TTS_VOICE=zh-CN-XiaoxiaoNeural
```

More docs:
- [QUICKSTART.md](QUICKSTART.md) (Chinese) â€“ practical recipes
- [SILICONFLOW.md](SILICONFLOW.md) (Chinese) â€“ SiliconFlow setup
- [.env.example](.env.example) â€“ full config template

### 3) Start the server

```bash
uvicorn main:app --host 0.0.0.0 --port 8000 --reload

# or
python main.py
```

### 4) Run a client

#### Option A: Terminal UI client (recommended)

```bash
pip install textual sounddevice
python push_to_talk_app.py
```

Notes:
- Speak directly to the microphone; Server VAD detects speech automatically
- Press **Q** to quit
- Default URL: `ws://localhost:8000/v1/realtime`
- You can set `USE_LOCAL_SERVER = False` inside the client to use OpenAI instead

#### Option B: Simple test client

```bash
python test_client.py
python test_client.py -i
```

#### Option C: Use OpenAI SDK (pointing to this server)

```python
from openai import AsyncOpenAI

client = AsyncOpenAI(
    base_url="http://localhost:8000/v1",
    api_key="dummy-key"  # no real key needed for local server
)

async with client.realtime.connect(model="gpt-realtime") as conn:
    ...
```

## ğŸ”§ Architecture

### Data flow

```
Client â†’ OpenAI-style JSON â†’ Transport (translate) â†’ Pipeline
                                            â†“
Client â† OpenAI-style JSON â† Transport (translate) â† (VAD â†’ STT â†’ LLM â†’ TTS)
```

### Key components

1. **Transport** ([transport.py](transport.py))
   - Converts OpenAI-style events to internal frames and back

2. **Pipeline Manager** ([pipeline_manager.py](pipeline_manager.py))
   - VAD / STT / LLM / TTS orchestration

3. **Session Manager** ([realtime_session.py](realtime_session.py))
   - WebSocket session lifecycle; connects Transport â†” Pipeline

4. **Audio Utilities** ([audio_utils.py](audio_utils.py))
   - Audio resampling (24kHz â†” 16kHz)
   - Audio buffer management
   - Async audio player for client

## ğŸ“Š Supported Services

### STT (Speech-to-Text)
| Provider | Config Value | Notes | API Key |
|----------|--------------|-------|---------|
| **Deepgram** ğŸŒŸ | `deepgram` | High quality, 200 min/month free | Required |
| OpenAI Whisper | `openai_whisper` | OpenAI official API | Required |
| Local Whisper | `local_whisper` | Completely free, needs model download | Not required |

### LLM (Language Model)
| Provider | Config Value | Notes | API Key |
|----------|--------------|-------|---------|
| **SiliconFlow** ğŸŒŸ | `siliconflow` | Fast in China, ~1/10 OpenAI price | Required |
| OpenAI | `openai` | GPT-4o and other models | Required |
| Ollama | `ollama` | Local, completely free | Not required |

### TTS (Text-to-Speech)
| Provider | Config Value | Notes | API Key |
|----------|--------------|-------|---------|
| **Edge TTS** ğŸŒŸ | `edge_tts` | Microsoft Edge TTS, free | Not required |
| ElevenLabs | `elevenlabs` | High quality, 10k chars/month free | Required |
| OpenAI TTS | `openai_tts` | OpenAI official TTS | Required |

ğŸŒŸ = Recommended

## ğŸ“ Configuration

All configuration is done via `.env` file. See [.env.example](.env.example) for the complete template.

### VAD Configuration (Open-mic mode)
```bash
VAD_THRESHOLD=0.5          # Sensitivity (0.0-1.0), higher = less sensitive
VAD_SILENCE_DURATION_MS=500  # Silence detection duration (ms)
VAD_PREFIX_PADDING_MS=300    # Speech prefix padding (ms)
```

## ğŸ“„ License

See [LICENSE](LICENSE).
